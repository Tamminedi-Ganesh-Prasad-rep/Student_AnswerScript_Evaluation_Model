{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers scikit-learn nltk\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.stem import PorterStemmer\n",
        "import random\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "tnW-bct6Ufp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "def grade_to_class(grade):\n",
        "    return int(grade * 2)\n",
        "\n",
        "def class_to_grade(class_idx):\n",
        "    return class_idx / 2.0\n",
        "\n",
        "def contains_grammar_mistakes(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = [tag for word, tag in pos_tag(tokens)]\n",
        "\n",
        "    patterns = [\n",
        "        ['JJ', 'JJ'],\n",
        "        ['NN', 'VB'],\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        for i in range(len(tags) - len(pattern) + 1):\n",
        "            if tags[i:i+len(pattern)] == pattern:\n",
        "                return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "fSq0VO-TUfsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_answers = [\n",
        "    \"Supervised learning requires labeled data, which means each example in the dataset is paired with the correct output. It uses this labeled data to train the model and make predictions on new, unseen data. Unsupervised learning, on the other hand, deals with unlabeled data. It tries to find patterns or relationships in the data without any prior knowledge of the correct output.\",\n",
        "    \"Supervised learning uses labeled data for training, whereas unsupervised learning uses unlabeled data.\",\n",
        "    \"In supervised learning, we have an algorithm that learns from labeled data, while unsupervised learning is where the algorithm learns from unlabeled data.\",\n",
        "    \"Supervised is when the data is labeled. Unsupervised is when it's not.\",\n",
        "    \"Supervised learning is where you train the model by presenting it with input and the correct output, and it learns by comparing its actual output with the correct outputs to find errors. Unsupervised learning is where a model is trained using no historical data.\"\n",
        "]\n",
        "\n",
        "grades = [5.0, 3.5, 4.0, 2.5, 4.5]\n",
        "\n",
        "answers, grades_dataset = zip(*random.choices(list(zip(sample_answers, grades)), k=1000))\n",
        "grades_dataset = [grade + random.uniform(-0.5, 0.5) for grade in grades_dataset]\n",
        "preprocessed_answers = [preprocess_text(ans) for ans in answers]\n",
        "final_answers = preprocessed_answers * 2\n",
        "final_grades = grades_dataset * 2\n",
        "\n",
        "train_answers, test_answers, train_grades, test_grades = train_test_split(\n",
        "    final_answers, final_grades, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_grades = [grade_to_class(grade) for grade in train_grades]\n",
        "test_grades = [grade_to_class(grade) for grade in test_grades]\n",
        "\n",
        "train_inputs = tokenizer(train_answers, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "test_inputs = tokenizer(test_answers, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "train_labels = torch.tensor(train_grades).long()\n",
        "test_labels = torch.tensor(test_grades).long()\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs.input_ids, test_inputs.attention_mask, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "oKp-AAjJUfuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict(models, tokenized_input):\n",
        "    models = [model.eval() for model in models]\n",
        "    total_prediction = torch.zeros(tokenized_input[\"input_ids\"].shape[0], 11)  # For 11 classes\n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            outputs = model(**tokenized_input)\n",
        "            total_prediction += outputs.logits\n",
        "    return torch.argmax(total_prediction, dim=1)\n",
        "\n",
        "model1 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=11)\n",
        "model2 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=11)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer1 = AdamW(model1.parameters(), lr=2e-5)\n",
        "optimizer2 = AdamW(model2.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "N1Ka-ysjUfxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, optimizer in [(model1, optimizer1), (model2, optimizer2)]:\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "M89Q5-KAUfzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "LvnCLA546VP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feedback(student_answer, keywords=None, point_based=False):\n",
        "    # Check for out-of-context answers\n",
        "    if is_out_of_context(student_answer, sample_answers, keywords):\n",
        "        feedback = \"Your answer is out of context or not relevant to the question.\"\n",
        "        return feedback, 0.0\n",
        "\n",
        "    preprocessed_student_answer = preprocess_text(student_answer)\n",
        "    tokenized_student_answer = tokenizer(preprocessed_student_answer, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "    predicted_class = ensemble_predict([model1, model2], tokenized_student_answer).item()\n",
        "    predicted_score = class_to_grade(predicted_class) / 2.5  # Adjust to 0-2 scale\n",
        "\n",
        "    # Deduct marks for very short answers\n",
        "    if len(student_answer.split()) < 5:\n",
        "        predicted_score = max(predicted_score - 0.5, 0)\n",
        "\n",
        "    # Deduct marks for grammar mistakes\n",
        "    if contains_grammar_mistakes(student_answer):\n",
        "        predicted_score = max(predicted_score - 0.25, 0)\n",
        "\n",
        "    # Point-based scoring\n",
        "    if point_based:\n",
        "        points = [is_out_of_context(point, sample_answers) for point in student_answer.split('.')]\n",
        "        valid_points = sum([1 for point in points if not point])\n",
        "        predicted_score = min(valid_points, 2.0)\n",
        "\n",
        "    feedback_thresholds = [\n",
        "        (2.0, \"Perfect! Your answer is spot on.\"),\n",
        "        (1.5, \"Very good! You've covered most of the key points.\"),\n",
        "        (1.0, \"Good job! You've covered some of the main points.\"),\n",
        "        (0.5, \"You've touched upon the topic, but you need to elaborate more.\"),\n",
        "        (0.0, \"Your answer doesn't address the question.\")\n",
        "    ]\n",
        "\n",
        "    feedback = \"Please review your answer.\"\n",
        "    for threshold, message in feedback_thresholds:\n",
        "        if predicted_score >= threshold:\n",
        "            feedback = message\n",
        "            break\n",
        "\n",
        "    return feedback, predicted_score\n",
        "\n",
        "# Keywords and point-based instructions for the question\n",
        "keywords_for_question = [\"supervised\", \"unsupervised\", \"learning\"]\n",
        "point_based_question = True\n",
        "\n",
        "student_answer = input(\"Enter your response: \")\n",
        "feedback, score = get_feedback(student_answer, keywords_for_question, point_based_question)\n",
        "print(f\"Predicted Grade: {score}\")\n",
        "print(\"Feedback:\", feedback)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2M3pWIIUrA3",
        "outputId": "0455f7f1-a91e-4304-dc66-e30bb2c6ba2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your response: Unsupervised learning:- No help at all is provided, the model should learn everything on its own, Supervised learning:- Humans oversee and help the model to learn\n",
            "Predicted Grade: 1\n",
            "Feedback: Good job! You've covered some of the main points.\n"
          ]
        }
      ]
    }
  ]
}